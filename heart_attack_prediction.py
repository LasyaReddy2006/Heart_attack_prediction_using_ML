# -*- coding: utf-8 -*-
"""ml project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rMkrIL3pIT2DVS89kPaZencm4whG62gq
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt

data = pd.read_csv("./Medicaldataset.csv")

data.columns = data.columns.str.strip()

le = LabelEncoder()
data['Result'] = le.fit_transform(data['Result'])

target_col = 'Result'
class_counts = data[target_col].value_counts()
print("\nClass distribution:\n", class_counts)
imbalance_ratio = class_counts.min() / class_counts.max()
print(f"\nClass imbalance ratio: {imbalance_ratio:.2f}")
if imbalance_ratio < 0.5:
    print(" Warning: Significant class imbalance detected!")

X = data.drop(target_col, axis=1)
y = data[target_col]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, stratify=y, random_state=5
)

base_models = [
    ('svm2', SVC(
        C=5, kernel='rbf', gamma='scale', probability=True, random_state=5
    )),
    ('xgb', XGBClassifier(
        n_estimators=300, max_depth=5, learning_rate=0.05,
        subsample=0.8, colsample_bytree=0.8,
        use_label_encoder=False, eval_metric='logloss', random_state=5
    )),
    ('svc', SVC(
        C=10, kernel='rbf', gamma=0.1, probability=True, random_state=5
    ))
]

meta_model = XGBClassifier(
    n_estimators=200, max_depth=3, learning_rate=0.1,
    subsample=0.8, colsample_bytree=0.8, random_state=5
)

stacking_model = StackingClassifier(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5,
    n_jobs=-1,
    passthrough=True
)

stacking_model.fit(X_train, y_train)

y_pred = stacking_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"\n Stacking Model Accuracy: {accuracy * 100:.2f}%")
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

K = 5
cv_scores = cross_val_score(stacking_model, X_scaled, y, cv=K)
print(f"\n{K}-Fold Cross-Validation Accuracy: {cv_scores.mean() * 100:.2f}%")

best_xgb = base_models[1][1]
best_xgb.fit(X_train, y_train)
importances = best_xgb.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10,6))
plt.title("XGBoost Feature Importances")
plt.bar(range(X.shape[1]), importances[indices])
plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)
plt.show()

sample_input = [[48,1,135,98,60,100,94.79,0.004]]
sample_scaled = scaler.transform(sample_input)
sample_pred = stacking_model.predict(sample_scaled)

print("\nSample prediction (1 = Positive, 0 = Negative):", sample_pred[0])

# âœ… Heart Attack / Cardiac Event Prediction Pipeline

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt

# Step 1: Load dataset
data = pd.read_csv("./Medicaldataset.csv")

# Step 2: Strip column names
data.columns = data.columns.str.strip()

# Step 3: Encode target column (Result)
le = LabelEncoder()
data['Result'] = le.fit_transform(data['Result'])

# Step 4: Check class imbalance
target_col = 'Result'
class_counts = data[target_col].value_counts()
print("\nClass distribution:\n", class_counts)
imbalance_ratio = class_counts.min() / class_counts.max()
print(f"\nClass imbalance ratio: {imbalance_ratio:.2f}")
if imbalance_ratio < 0.5:
    print("âš ï¸ Warning: Significant class imbalance detected!")

# Step 5: Separate features and target
X = data.drop(target_col, axis=1)
y = data[target_col]

# Step 6: Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 7: Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, stratify=y, random_state=5
)

# Step 8: Define base models (RF removed, SVM2 added)
base_models = [
    ('svm2', SVC(
        C=5, kernel='rbf', gamma='scale', probability=True, random_state=5
    )),
    ('xgb', XGBClassifier(
        n_estimators=300, max_depth=5, learning_rate=0.05,
        subsample=0.8, colsample_bytree=0.8,
        use_label_encoder=False, eval_metric='logloss', random_state=5
    )),
    ('svc', SVC(
        C=10, kernel='rbf', gamma=0.1, probability=True, random_state=5
    ))
]

# Step 9: Meta-model
meta_model = XGBClassifier(
    n_estimators=200, max_depth=3, learning_rate=0.1,
    subsample=0.8, colsample_bytree=0.8, random_state=5
)

# Step 10: Stacking Classifier
stacking_model = StackingClassifier(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5,
    n_jobs=-1,
    passthrough=True
)

# Step 11: Train
stacking_model.fit(X_train, y_train)

# Step 12: Evaluate
y_pred = stacking_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"\nðŸŽ¯ Stacking Model Accuracy: {accuracy * 100:.2f}%")
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Step 13: K-Fold Cross-Validation
K = 10
cv_scores = cross_val_score(stacking_model, X_scaled, y, cv=K)
print(f"\nðŸ”„ {K}-Fold Cross-Validation Accuracy: {cv_scores.mean() * 100:.2f}%")

# Step 14: Feature importance (using XGB base model)
best_xgb = base_models[1][1]
best_xgb.fit(X_train, y_train)
importances = best_xgb.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10,6))
plt.title("XGBoost Feature Importances")
plt.bar(range(X.shape[1]), importances[indices])
plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)
plt.show()

# Step 15: Sample prediction
sample_input = [[48,1,135,98,60,100,94.79,0.004]]
sample_scaled = scaler.transform(sample_input)
sample_pred = stacking_model.predict(sample_scaled)

print("\nSample prediction (1 = Positive, 0 = Negative):", sample_pred[0])

# âœ… Heart Attack / Cardiac Event Prediction Pipeline

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt

# Step 1: Load dataset
data = pd.read_csv("./Medicaldataset.csv")

# Step 2: Strip column names
data.columns = data.columns.str.strip()

# Step 3: Encode target column (Result)
le = LabelEncoder()
data['Result'] = le.fit_transform(data['Result'])

# Step 4: Check class imbalance
target_col = 'Result'
class_counts = data[target_col].value_counts()
print("\nClass distribution:\n", class_counts)
imbalance_ratio = class_counts.min() / class_counts.max()
print(f"\nClass imbalance ratio: {imbalance_ratio:.2f}")
if imbalance_ratio < 0.5:
    print("âš ï¸ Warning: Significant class imbalance detected!")

# Step 5: Separate features and target
X = data.drop(target_col, axis=1)
y = data[target_col]

# Step 6: Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 7: Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, stratify=y, random_state=5
)

# Step 8: Define base models (RF removed, SVM2 added)
base_models = [
    ('svm2', SVC(
        C=5, kernel='rbf', gamma='scale', probability=True, random_state=5
    )),
    ('xgb', XGBClassifier(
        n_estimators=300, max_depth=5, learning_rate=0.05,
        subsample=0.8, colsample_bytree=0.8,
        use_label_encoder=False, eval_metric='logloss', random_state=5
    )),
    ('svc', SVC(
        C=10, kernel='rbf', gamma=0.1, probability=True, random_state=5
    ))
]

# Step 9: Meta-model
meta_model = XGBClassifier(
    n_estimators=200, max_depth=3, learning_rate=0.1,
    subsample=0.8, colsample_bytree=0.8, random_state=5
)

# Step 10: Stacking Classifier
stacking_model = StackingClassifier(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5,
    n_jobs=-1,
    passthrough=True
)

# Step 11: Train
stacking_model.fit(X_train, y_train)

# Step 12: Evaluate
y_pred = stacking_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"\n Stacking Model Accuracy: {accuracy * 100:.2f}%")
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Step 13: K-Fold Cross-Validation
K = 15
cv_scores = cross_val_score(stacking_model, X_scaled, y, cv=K)
print(f"\nðŸ”„ {K}-Fold Cross-Validation Accuracy: {cv_scores.mean() * 100:.2f}%")

# Step 14: Feature importance (using XGB base model)
best_xgb = base_models[1][1]
best_xgb.fit(X_train, y_train)
importances = best_xgb.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10,6))
plt.title("XGBoost Feature Importances")
plt.bar(range(X.shape[1]), importances[indices])
plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)
plt.show()

# Step 15: Sample prediction
sample_input = [[48,1,135,98,60,100,94.79,0.004]]
sample_scaled = scaler.transform(sample_input)
sample_pred = stacking_model.predict(sample_scaled)

print("\nSample prediction (1 = Positive, 0 = Negative):", sample_pred[0])